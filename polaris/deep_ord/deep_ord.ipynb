{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1741635906432,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "PrX_hqdKD8H7"
   },
   "outputs": [],
   "source": [
    "# proj_dir =  'drive/MyDrive/Polaris_ASAP_competition/polaris_challenge/admet'\n",
    "proj_dir = '/Users/robertarbon/Library/CloudStorage/GoogleDrive-robert.arbon@gmail.com/My Drive/Polaris_ASAP_competition/polaris_challenge/admet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4234,
     "status": "ok",
     "timestamp": 1741635912487,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "FamOIzIxB9TS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacecutter.models import OrdinalLogisticModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import datamol as dm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import SkorchDoctor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from spacecutter.callbacks import AscensionCallback\n",
    "from spacecutter.losses import CumulativeLinkLoss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# from molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV1UjUAUCOCf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5268,
     "status": "ok",
     "timestamp": 1741635917757,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "7rgAx5oEB9TW"
   },
   "outputs": [],
   "source": [
    "# Imputed training data\n",
    "df_imp = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_log_pmm_imputed.csv')\n",
    "# Non-imputed validation data\n",
    "df_val = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_features.csv')\n",
    "# change names\n",
    "df_val.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "df_imp.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "# Smiles columns because they were removed (for some unknown reason)\n",
    "df_smiles = pd.read_csv(f'{proj_dir}/data/train_admet_all.csv')\n",
    "df_smiles.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "\n",
    "df_imp = df_imp.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')\n",
    "df_val = df_val.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1741636483590,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "JRfXbaZGB9TY",
    "outputId": "55e3c7e8-19eb-4f75-96b0-cd9d96e499f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "using chemberta\n",
      "\tcreating new scaler\n",
      "using chemprop features\n",
      "\tcreating new scaler\n",
      "using rdkit simple\n",
      "\tcreating new scaler\n",
      "validation data\n",
      "using chemberta\n",
      "\tusing existing scaler\n",
      "using chemprop features\n",
      "\tusing existing scaler\n",
      "using rdkit simple\n",
      "\tusing existing scaler\n",
      "LogD\n",
      "\t0/1,\tmae: 0.36\n",
      "\tmean epoches: 29.0\n",
      "LogMLM\n",
      "\t0/1,\tmae: 0.46\n",
      "\tmean epoches: 1.0\n",
      "LogHLM\n",
      "\t0/1,\tmae: 0.67\n",
      "\tmean epoches: 30.0\n",
      "LogKSOL\n",
      "\t0/1,\tmae: 0.53\n",
      "\tmean epoches: 2.0\n",
      "LogMDR1.MDCKII\n",
      "\t0/1,\tmae: 0.64\n",
      "\tmean epoches: 2.0\n"
     ]
    }
   ],
   "source": [
    "train, val = train_data(df_train=df_imp,\n",
    "                        imp_ix=1,\n",
    "                        df_val=df_val,\n",
    "                        n_cuts=None, features=['chemberta', 'chem_prop', 'rdkit_simple'])\n",
    "\n",
    "targets = list(train[1].keys())\n",
    "\n",
    "training_by_target = {}\n",
    "\n",
    "for target in targets:\n",
    "  num_ds = 1\n",
    "  ix_by_imp = train[2]\n",
    "  if ix_by_imp is not None:\n",
    "    num_ds = len(ix_by_imp)\n",
    "\n",
    "  # accumulators\n",
    "  all_y_hat = []\n",
    "  all_epochs = []\n",
    "\n",
    "  # All imputed training datasets\n",
    "  all_X = train[0]\n",
    "  all_y = train[1][target]['values'].reshape(-1, 1)\n",
    "\n",
    "  # only keep features which are different\n",
    "  keep_ix = np.std(all_X, axis=0)>0\n",
    "  all_X = all_X[:, keep_ix].astype(np.float32)\n",
    "\n",
    "  # Get validation data (not imputed so only done once. contains missing values)\n",
    "  missing_ix = val[1][target]['missing_ix']\n",
    "  Xval = val[0]\n",
    "\n",
    "  Xval = Xval[missing_ix, :].astype(np.float32)\n",
    "  Xval = Xval[:, keep_ix]\n",
    "  # validation y values have been digitized using all the training y values.\n",
    "  # so should be consistent.\n",
    "  yval = val[1][target]['values'].reshape(-1, 1)\n",
    "  bins = train[1][target]['bins']\n",
    "\n",
    "  print(f'{target}')\n",
    "  for i in range(num_ds):\n",
    "    if i % 10 == 0:\n",
    "      print(f'\\t{i}/{num_ds}', end=',')\n",
    "\n",
    "    # Get imputed training dataset.\n",
    "    imp_ix = ix_by_imp[i+1] if ix_by_imp is not None else np.arange(all_X.shape[0]) # The zeroth imputed dataset is the original data.\n",
    "    X = all_X[imp_ix, :]\n",
    "    y = train[1][target]['values'].reshape(-1, 1)\n",
    "    y = y[imp_ix]\n",
    "\n",
    "    # print(f\"{target}:\\n\\tn_train_obs: {X.shape[0]}, n_val_obs: {Xval.shape[0]} n_preds: {X.shape[1]}\")\n",
    "\n",
    "    # Stack all data for convenience.\n",
    "    train_v_X = np.vstack([X, Xval])\n",
    "    train_v_y = np.vstack([y, yval])\n",
    "    train_ix = np.arange(X.shape[0])\n",
    "    val_ix = np.arange(X.shape[0], train_v_X.shape[0])\n",
    "\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    predictor = nn.Sequential(\n",
    "        nn.Linear(num_features, num_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features, num_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "    skorch_model = NeuralNet(\n",
    "        module=OrdinalLogisticModel,\n",
    "        module__predictor=predictor,\n",
    "        module__num_classes=num_classes,\n",
    "        criterion=CumulativeLinkLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        train_split=lambda ds, y: (torch.utils.data.Subset(ds, train_ix),\n",
    "                                  torch.utils.data.Subset(ds, val_ix)),\n",
    "        callbacks=[\n",
    "            ('ascension', AscensionCallback()),\n",
    "            ('early_stopping', EarlyStopping(threshold=0.0001, load_best=True,\n",
    "                                            patience=100))\n",
    "        ],\n",
    "        verbose=0,\n",
    "        batch_size=X.shape[0],\n",
    "        max_epochs=500,\n",
    "\n",
    "    )\n",
    "\n",
    "    skorch_model.fit(train_v_X, train_v_y)\n",
    "\n",
    "\n",
    "    y_hat = np.argmax(skorch_model.predict(Xval), axis=1)\n",
    "\n",
    "\n",
    "    all_y_hat.append(bins[y_hat])\n",
    "\n",
    "    for i in range(len(skorch_model.history)-1, -1, -1):\n",
    "      batch = skorch_model.history[i]\n",
    "\n",
    "      if batch['valid_loss_best']:\n",
    "        all_epochs.append(batch['epoch'])\n",
    "        break\n",
    "\n",
    "  # mean prediction over imputed datasets.\n",
    "  mean_y_hat = np.mean(np.vstack(all_y_hat), axis=0)\n",
    "  y_val_cont = bins[yval.reshape(-1)]\n",
    "\n",
    "  print(f\"\\tmae: {mean_absolute_error(y_val_cont, mean_y_hat):4.2f}\")\n",
    "  print(f\"\\tmean epoches: {np.mean(all_epochs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309924,
     "status": "ok",
     "timestamp": 1741622532708,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "JHRuE3q9niVs",
    "outputId": "8f396da6-c914-4181-c104-dee4ce321bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using rdkit simple\n",
      "using chemprop features\n",
      "using rdkit simple\n",
      "using chemprop features\n",
      "\tmae: 0.61\n",
      "\tepochs: 12/112\n",
      "\tmae: 0.45\n",
      "\tepochs: 6/106\n",
      "\tmae: 0.51\n",
      "\tepochs: 33/133\n",
      "\tmae: 0.59\n",
      "\tepochs: 20/120\n",
      "\tmae: 0.87\n",
      "\tepochs: 23/123\n"
     ]
    }
   ],
   "source": [
    "# train, val = train_data(df_train=df_imp,\n",
    "#                         imp_ix=None,\n",
    "#                         df_val=df_val,\n",
    "#                         n_cuts=None, features=['rdkit_simple', 'chemp_prop'])\n",
    "\n",
    "# targets = list(train[1].keys())\n",
    "\n",
    "# training_by_target = {}\n",
    "# n_imp_ds = df_imp['.imp'].max()\n",
    "\n",
    "# for target in targets:\n",
    "#   all_X = train[0]\n",
    "#   all_y = train[1][target]['values'].reshape(-1, 1)\n",
    "\n",
    "#   # only keep features which are different\n",
    "#   keep_ix = np.std(all_X, axis=0)>0\n",
    "#   all_X = all_X[:, keep_ix].astype(np.float32)\n",
    "\n",
    "#   # Get imputed training dataset.\n",
    "#   shuffle_ix = np.random.choice(np.arange(all_X.shape[0]), size=all_X.shape[0], replace=False)\n",
    "#   X = all_X[shuffle_ix, :]\n",
    "#   y = all_y[shuffle_ix]\n",
    "\n",
    "#   # Get validation data (not imputed so only done once. contains missing values)\n",
    "#   missing_ix = val[1][target]['missing_ix']\n",
    "#   Xval = val[0]\n",
    "#   Xval = Xval[missing_ix, :].astype(np.float32)\n",
    "#   Xval = Xval[:, keep_ix]\n",
    "#   # validation y values have been digitized using all the training y values.\n",
    "#   # so should be consistent.\n",
    "#   yval = val[1][target]['values'].reshape(-1, 1)\n",
    "#   bins = train[1][target]['bins']\n",
    "\n",
    "#   # batch size\n",
    "#   batch_size = all_X.shape[0]//n_imp_ds\n",
    "\n",
    "#   # Stack all data for convenience.\n",
    "#   train_v_X = np.vstack([X, Xval])\n",
    "#   train_v_y = np.vstack([y, yval])\n",
    "#   train_ix = np.arange(X.shape[0])\n",
    "#   val_ix = np.arange(X.shape[0], train_v_X.shape[0])\n",
    "\n",
    "#   # Model dimensions\n",
    "#   num_features = X.shape[1]\n",
    "#   num_classes = len(np.unique(y))\n",
    "\n",
    "#   # Simple predictor\n",
    "#   predictor = nn.Sequential(\n",
    "#     nn.Linear(num_features, num_features),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(num_features, num_features),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(num_features, 1),\n",
    "#   )\n",
    "\n",
    "#   # Model\n",
    "#   skorch_model = NeuralNet(\n",
    "#     module=OrdinalLogisticModel,\n",
    "#     module__predictor=predictor,\n",
    "#     module__num_classes=num_classes,\n",
    "#     criterion=CumulativeLinkLoss,\n",
    "#     optimizer=torch.optim.Adam,\n",
    "#     train_split=lambda ds, y: (torch.utils.data.Subset(ds, train_ix),\n",
    "#                               torch.utils.data.Subset(ds, val_ix)),\n",
    "#     callbacks=[\n",
    "#         ('ascension', AscensionCallback()),\n",
    "#         ('early_stopping', EarlyStopping(threshold=0.0001, load_best=True,\n",
    "#                                         patience=100))\n",
    "#     ],\n",
    "#     verbose=0,\n",
    "#     batch_size=X.shape[0],\n",
    "#     max_epochs=500,\n",
    "\n",
    "#   )\n",
    "#   # Fit\n",
    "#   skorch_model.fit(train_v_X, train_v_y)\n",
    "\n",
    "#   # predict on validation\n",
    "#   y_hat_ord = np.argmax(skorch_model.predict(Xval), axis=1)\n",
    "#   y_hat = bins[y_hat_ord]\n",
    "#   y_val = bins[yval.reshape(-1)]\n",
    "\n",
    "#   print(f\"\\tmae: {mean_absolute_error(y_val, y_hat):4.2f}\")\n",
    "\n",
    "#   # Find best epoch\n",
    "#   for i in range(len(skorch_model.history)-1, -1, -1):\n",
    "#     batch = skorch_model.history[i]\n",
    "\n",
    "#     if batch['valid_loss_best']:\n",
    "#       print(f\"\\tepochs: {batch['epoch']}/{len(skorch_model.history)}\")\n",
    "#       break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spacecutter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
